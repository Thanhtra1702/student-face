from flask import Flask, render_template, Response, jsonify, request
import cv2
import threading
import time
import datetime
import os
import numpy as np
from deepface import DeepFace
from qdrant_client import QdrantClient

# Import DB handler t·ª´ module c≈©
from kiosk_db import DatabaseHandler

import mediapipe as mp

import signal
import sys

app = Flask(__name__)

# --- PRELOAD MODEL (ƒê·ªÉ kh·ªüi ƒë·ªông nhanh h∆°n) ---
print("üöÄ ƒêang t·∫£i model AI...")
try:
    # Preload ArcFace model b·∫±ng c√°ch t·∫°o embedding gi·∫£
    import numpy as np
    dummy_img = np.zeros((112, 112, 3), dtype=np.uint8)
    DeepFace.represent(img_path=dummy_img, model_name="ArcFace", detector_backend="skip", enforce_detection=False)
    print("‚úÖ Model AI ƒë√£ s·∫µn s√†ng!")
except:
    pass

# --- QDRANT CLIENT ---
QDRANT_PATH = "./qdrant_db"
COLLECTION_NAME = "student_faces"
# qdrant_client = QdrantClient(path=QDRANT_PATH) # REMOVED to avoid double locking

# --- GLOBAL STATE ---
class KioskState:
    def __init__(self):
        self.frame = None
        self.clean_snapshot = None # B·∫£n ·∫£nh c·ª±c s·∫°ch ƒë·ªÉ l∆∞u DB
        self.lock = threading.Lock()
        self.status = "SCANNING"  # SCANNING, PROCESSING, CONFIRM, SUCCESS
        self.progress = 0
        self.student_data = None 
        self.last_scan_time = 0
        self.process_start_time = 0
        self.db = DatabaseHandler()
        self.running = True # C·ªù ki·ªÉm so√°t v√≤ng l·∫∑p
        # Liveness Blink State
        self.blink_counter = 0
        self.is_blinking = False
        self.last_blink_time = 0  
        # Verification State
        self.consecutive_match_count = 0
        self.last_recognized_sid = None
        self.is_near = False # Tr·∫°ng th√°i kho·∫£ng c√°ch m·ªõi
        
state = KioskState()

# Handle Ctrl+C
def signal_handler(sig, frame):
    print('üëã ƒêang t·∫Øt h·ªá th·ªëng NGAY L·∫¨P T·ª®C...')
    state.running = False
    # time.sleep(0.5)  <-- X√≥a d√≤ng n√†y
    os._exit(0)  # Force exit ngay l·∫≠p t·ª©c

signal.signal(signal.SIGINT, signal_handler)

# --- BLINK DETECTION HELPERS ---
mp_face_mesh = mp.solutions.face_mesh
# Index m·∫Øt tr√°i/ph·∫£i trong FaceMesh (Chu·∫©n Mediapipe)
LEFT_EYE = [362, 385, 387, 263, 373, 380]
RIGHT_EYE = [33, 160, 158, 133, 153, 144]

def calculate_ear(landmarks, eye_indices, w, h):
    """T√≠nh Eye Aspect Ratio (T·ª∑ l·ªá m·ªü m·∫Øt)"""
    # L·∫•y t·ªça ƒë·ªô
    coords = []
    for idx in eye_indices:
        lm = landmarks[idx]
        coords.append((lm.x * w, lm.y * h))
    
    # T√≠nh kho·∫£ng c√°ch d·ªçc (Vertical)
    v1 = np.linalg.norm(np.array(coords[1]) - np.array(coords[5]))
    v2 = np.linalg.norm(np.array(coords[2]) - np.array(coords[4]))
    
    # T√≠nh kho·∫£ng c√°ch ngang (Horizontal)
    h_dist = np.linalg.norm(np.array(coords[0]) - np.array(coords[3]))
    
    ear = (v1 + v2) / (2.0 * h_dist)
    return ear

# --- AI ENHANCEMENT HELPERS ---
def preprocess_frame(frame):
    """
    S·ª≠ d·ª•ng CLAHE ƒë·ªÉ c√¢n b·∫±ng ƒë·ªô t∆∞∆°ng ph·∫£n, gi√∫p AI nh·∫≠n di·ªán t·ªët h∆°n 
    trong ƒëi·ªÅu ki·ªán √°nh s√°ng y·∫øu ho·∫∑c b·ªã ng∆∞·ª£c s√°ng.
    """
    try:
        lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        # C√¢n b·∫±ng s√°ng (CLAHE) - M·ª©c 3.0 l√† t·ªëi ∆∞u nh·∫•t cho HD
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        cl = clahe.apply(l)
        limg = cv2.merge((cl, a, b))
        return cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)
    except:
        return frame

def check_spoofing_opencv(frame, face_area=None):
    return False, "Real"

def run_recognition_async(face_crop, full_frame, state, x_min, y_min, x_max, y_max):
    """Ch·∫°y AI Nh·∫≠n di·ªán - S·ª≠ d·ª•ng ·∫£nh crop ƒë·ªÉ x·ª≠ l√Ω nh∆∞ng d√πng ·∫£nh g·ªëc ƒë·ªÉ hi·ªÉn th·ªã"""
    # face_crop l√∫c n√†y ƒë√£ l√† v√πng ƒë∆∞·ª£c crop t·ª´ camera_worker (Zoomed face)
    try:
        input_frame = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
        
        # --- 1. DETECT & EXTRACT FACE ---
        face_objs = DeepFace.extract_faces(
            img_path=input_frame,
            detector_backend="mediapipe",
            enforce_detection=False, # ƒê√£ crop s·∫µn n√™n kh√¥ng c·∫ßn g·∫Øt gao detection
            align=True,
            grayscale=False
        )
        
        if not face_objs:
            with state.lock:
                state.status = "SCANNING"
                state.progress = 0
            return

        current_face = face_objs[0]["face"]
        if current_face.max() <= 1.0:
            current_face = (current_face * 255).astype(np.uint8)

        # --- 2. GET EMBEDDING ---
        results = DeepFace.represent(
            img_path=current_face,
            model_name="ArcFace",
            detector_backend="skip",
            enforce_detection=False,
            align=True
        )
        
        if not results: 
            with state.lock:
                state.status = "SCANNING"
                state.progress = 0
            return
            
        embedding = results[0]["embedding"]
        
        # --- 3. SEARCH DATABASE ---
        search_res = state.db.client.query_points(
            collection_name=COLLECTION_NAME,
            query=embedding,
            limit=3
        ).points
        
        found = False
        if search_res:
            best_match = search_res[0]
            score = best_match.score
            current_sid = best_match.payload['student_id']
            print(f"üéØ Top 1: {current_sid} - Score: {score:.4f}")
            
            accepted_sid = None
            # --- TRIPLE CHECK LOGIC (T·ªëi ∆∞u T·ªëc ƒë·ªô) ---
            # 1. Ng∆∞·ª°ng ƒëi·ªÉm c∆° b·∫£n (0.45 l√† m·ª©c c√¢n b·∫±ng nh·∫•t)
            is_passing_score = score > 0.45
            
            # 2. Gap Check (Gi·∫£m xu·ªëng 0.02 v√¨ ƒë√£ c√≥ x√°c nh·∫≠n 2 l·∫ßn)
            is_ambiguous = False
            competitor_score = 0
            for res in search_res[1:]:
                if res.payload['student_id'] != current_sid:
                    competitor_score = res.score
                    break
            
            if competitor_score > 0:
                gap = score - competitor_score
                if gap < 0.02 and score < 0.65: 
                    is_ambiguous = True
                    print(f"‚ö†Ô∏è Nh·∫≠p nh·∫±ng gi·ªØa {current_sid} v√† ng∆∞·ªùi kh√°c (Gap: {gap:.4f})")
            
            if is_passing_score and not is_ambiguous:
                accepted_sid = current_sid
            
            if accepted_sid:
                with state.lock:
                    if state.last_recognized_sid == accepted_sid:
                        state.consecutive_match_count += 1
                    else:
                        state.last_recognized_sid = accepted_sid
                        state.consecutive_match_count = 1
                    
                    print(f"üîÑ Kh·ªõp l·∫ßn {state.consecutive_match_count}/2 cho ID: {accepted_sid}")
                    
                    # --- FAST PATH: N·∫øu score > 0.65, x√°c nh·∫≠n ngay l·∫≠p t·ª©c ---
                    is_very_sure = score > 0.65
                    
                    if state.consecutive_match_count >= 2 or is_very_sure:
                        print(f"‚úÖ X√ÅC NH·∫¨N CH√çNH X√ÅC{' (FAST)' if is_very_sure else ''}: {accepted_sid}")
                        name, sch, room = state.db.get_student_info(accepted_sid)
                        state.student_data = {
                            "name": name,
                            "student_id": accepted_sid,
                            "schedule": sch,
                            "room": room,
                            "checkin_time": datetime.datetime.now().strftime("%H:%M %d/%m")
                        }
                        # --- SMART SNAPSHOT: L∆∞u 2 b·∫£n (B·∫£n ƒë·∫πp hi·ªÉn th·ªã v√† B·∫£n s·∫°ch l∆∞u DB) ---
                        # 1. L∆∞u b·∫£n s·∫°ch (Original HD)
                        state.clean_snapshot = full_frame.copy()
                        
                        # 2. V·∫Ω khung xanh l√™n b·∫£n copy ƒë·ªÉ hi·ªÉn th·ªã (thickness=3, length=40 cho HD)
                        display_frame = full_frame.copy()
                        t, l = 3, 40
                        cv2.line(display_frame, (x_min, y_min), (x_min + l, y_min), (73, 132, 30), t)
                        cv2.line(display_frame, (x_min, y_min), (x_min, y_min + l), (73, 132, 30), t)
                        cv2.line(display_frame, (x_max, y_min), (x_max - l, y_min), (73, 132, 30), t)
                        cv2.line(display_frame, (x_max, y_min), (x_max, y_min + l), (73, 132, 30), t)
                        cv2.line(display_frame, (x_min, y_max), (x_min + l, y_max), (73, 132, 30), t)
                        cv2.line(display_frame, (x_min, y_max), (x_min, y_max - l), (73, 132, 30), t)
                        cv2.line(display_frame, (x_max, y_max), (x_max - l, y_max), (73, 132, 30), t)
                        cv2.line(display_frame, (x_max, y_max), (x_max, y_max - l), (73, 132, 30), t)
                        
                        state.frame = display_frame
                        state.status = "CONFIRM"
                        state.progress = 100
                        state.consecutive_match_count = 0 
                        found = True
                    else:
                        state.status = "PROCESSING"
                        state.progress = 95
                        found = True
            else:
                print(f"‚ùå Low Score (< 0.45) ho·∫∑c Ambiguous")
        else:
            print("‚ùå DB Empty")
        
        if not found:
            with state.lock:
                state.status = "SCANNING"
                state.progress = 0

    except Exception as e:
        print(f"üî• AI Exception: {e}")
        with state.lock:
            state.status = "SCANNING"
            state.progress = 0
    
    if state.status == "PROCESSING":
        with state.lock:
            state.process_start_time = time.time() - 0.4 

# --- CAMERA THREAD ---
def camera_worker():
    cap = cv2.VideoCapture(0)
    # N√¢ng c·∫•p l√™n HD 720p
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
    
    # Init Face Mesh
    face_mesh = mp_face_mesh.FaceMesh(
        max_num_faces=1,
        refine_landmarks=True,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )

    while state.running:
        ret, frame = cap.read()
        if not ret: break
        frame = cv2.flip(frame, 1)
        
        # --- NEW: L∆∞u l·∫°i frame s·∫°ch ƒë·ªÉ l√†m snapshot ---
        raw_frame = frame.copy()
        
        # --- STATE MACHINE ---
        current_time = time.time()
        
        # --- MAIN FACE SELECTION (Anti-Crowd) ---
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb_frame)
        
        if results.multi_face_landmarks:
            h, w, _ = frame.shape
            screen_center_x, screen_center_y = w // 2, h // 2
            
            best_face_data = None
            max_focus_score = -1

            # Duy·ªát qua t·∫•t c·∫£ m·∫∑t ƒë·ªÉ t√¨m "Ng∆∞·ªùi ch·ªß tr√¨"
            for face_landmarks in results.multi_face_landmarks:
                # T√≠nh bounding box
                x_min, y_min = w, h
                x_max, y_max = 0, 0
                for lm in face_landmarks.landmark:
                    cx, cy = int(lm.x * w), int(lm.y * h)
                    x_min, y_min = min(x_min, cx), min(y_min, cy)
                    x_max, y_max = max(x_max, cx), max(y_max, cy)
                
                # T√≠nh ƒëi·ªÉm ∆∞u ti√™n (Di·ªán t√≠ch * ƒê·ªô trung t√¢m)
                area = (x_max - x_min) * (y_max - y_min)
                face_center_x = (x_min + x_max) / 2
                face_center_y = (y_min + y_max) / 2
                dist_to_center = ((face_center_x - screen_center_x)**2 + (face_center_y - screen_center_y)**2)**0.5
                
                # Heuristic: ∆Øu ti√™n m·∫∑t TO v√† G·∫¶N T√ÇM
                focus_score = area / (dist_to_center + 1) 
                
                if focus_score > max_focus_score:
                    max_focus_score = focus_score
                    best_face_data = (x_min, y_min, x_max, y_max)

            # Ch·ªâ v·∫Ω v√† x·ª≠ l√Ω khu√¥n m·∫∑t T·ªêT NH·∫§T
            if best_face_data:
                x_min, y_min, x_max, y_max = best_face_data
                
                # --- DISTANCE FILTER (720p Optimized) ---
                face_width = x_max - x_min
                is_near_enough = face_width > 180 # M·ªü r·ªông kho·∫£ng c√°ch (~2m - 2.5m)
                
                with state.lock:
                    state.is_near = is_near_enough
                
                # V·∫Ω box (HD Thickness)
                color = (255, 255, 255) # White
                if is_near_enough:
                    color = (33, 111, 242) # FPT Orange
                if state.status == "CONFIRM": 
                    color = (73, 132, 30) # Green
                
                # V·∫Ω g√≥c Corner HD (D√†y h∆°n m·ªôt ch√∫t ƒë·ªÉ s·∫Øc n√©t)
                t, l = 3, 40
                cv2.line(frame, (x_min, y_min), (x_min + l, y_min), color, t)
                cv2.line(frame, (x_min, y_min), (x_min, y_min + l), color, t)
                cv2.line(frame, (x_max, y_min), (x_max - l, y_min), color, t)
                cv2.line(frame, (x_max, y_min), (x_max, y_min + l), color, t)
                cv2.line(frame, (x_min, y_max), (x_min + l, y_max), color, t)
                cv2.line(frame, (x_min, y_max), (x_min, y_max - l), color, t)
                cv2.line(frame, (x_max, y_max), (x_max - l, y_max), color, t)
                cv2.line(frame, (x_max, y_max), (x_max, y_max - l), color, t)
                
                # Trigger Processing - CH·ªà KHI ƒê·ª¶ G·∫¶N
                if is_near_enough and state.status == "SCANNING" and (current_time - state.last_scan_time > 1.0):
                    # --- DIGITAL ZOOM (CROP FACE FOR AI) ---
                    # C·∫Øt v√πng m·∫∑t c√≥ th√™m 40% padding ƒë·ªÉ AI d·ªÖ nh·∫≠n di·ªán h∆°n t·ª´ xa
                    pad_w = int((x_max - x_min) * 0.4)
                    pad_h = int((y_max - y_min) * 0.4)
                    x1, y1 = max(0, x_min - pad_w), max(0, y_min - pad_h)
                    x2, y2 = min(w, x_max + pad_w), min(h, y_max + pad_h)
                    
                    face_crop = frame[y1:y2, x1:x2].copy()
                    
                    if face_crop.size > 0:
                        with state.lock:
                            state.status = "PROCESSING"
                            state.process_start_time = current_time
                            state.progress = 0
                            # L∆∞u face_crop ƒë·ªÉ thread AI s·ª≠ d·ª•ng
                            state.pending_crop = face_crop

        # 2. PROCESSING logic (S·ª≠ d·ª•ng frame ƒë√£ v·∫Ω box l√†m preview)
        if state.status == "PROCESSING":
            elapsed = current_time - state.process_start_time
            if elapsed < 0:
                with state.lock: state.progress = 90 + int((current_time * 10) % 9)
            else:
                # Gi·∫£m th·ªùi gian ch·ªù xu·ªëng 0.1s ƒë·ªÉ c·∫£m gi√°c nhanh h∆°n
                prog = int((elapsed / 0.2) * 90)
                with state.lock: state.progress = min(90, max(0, prog))
                if elapsed > 0.1:
                    # L·∫•y v√πng ·∫£nh m·∫∑t ƒë√£ crop t·ª´ state
                    with state.lock:
                        ai_input = getattr(state, 'pending_crop', None)
                    
                    if ai_input is not None:
                        # Ti·ªÅn x·ª≠ l√Ω (C√¢n b·∫±ng s√°ng)
                        processed_ai_frame = preprocess_frame(ai_input.copy())
                        # Truy·ªÅn ·∫£nh S·∫†CH (raw_frame) ƒë·ªÉ v·∫Ω khung xanh khi kh√≥a frame
                        threading.Thread(target=run_recognition_async, 
                                       args=(processed_ai_frame, raw_frame.copy(), state, x_min, y_min, x_max, y_max), 
                                       daemon=True).start()
                        with state.lock: 
                            state.process_start_time = current_time + 1000 
                            state.pending_crop = None # Clear sau khi g·ª≠i
                    else:
                        with state.lock: state.status = "SCANNING"

        # --- UPDATE FRAME (Ch·ªâ update n·∫øu kh√¥ng ·ªü tr·∫°ng th√°i CONFIRM) ---
        if state.status != "CONFIRM":
            with state.lock:
                state.frame = frame.copy()
        
        time.sleep(0.005) 

# Start Thread
t = threading.Thread(target=camera_worker, daemon=True)
t.start()

# --- WEB ROUTES ---
@app.route('/')
def index():
    return render_template('index.html')

def generate_frames():
    while True:
        with state.lock:
            if state.frame is None: continue
            
            # encode frame
            _, buffer = cv2.imencode('.jpg', state.frame)
            frame_bytes = buffer.tobytes()
            
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')

@app.route('/video_feed')
def video_feed():
    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/api/status')
def get_status():
    with state.lock:
        return jsonify({
            "status": state.status,
            "progress": state.progress,
            "data": state.student_data,
            "is_near": state.is_near
        })

@app.route('/api/action', methods=['POST'])
def handle_action():
    req = request.json
    action = req.get('action') # 'confirm' or 'reject'
    
    if action == 'confirm':
        # Logic L∆∞u ƒëi·ªÉm danh (ƒê√£ c√≥)
        if state.student_data:
            sid = state.student_data['student_id']
            
            # --- T√çNH NƒÇNG T·ª∞ H·ªåC (SELF-LEARNING) ---
            # L∆∞u ·∫£nh x√°c th·ª±c v√†o collected_faces/MSSV/ ƒë·ªÉ ƒë·ªãnh k·ª≥ training l·∫°i
            try:
                # T·∫°o folder ri√™ng cho t·ª´ng sinh vi√™n
                student_collect_dir = os.path.join("collected_faces", sid)
                if not os.path.exists(student_collect_dir):
                    os.makedirs(student_collect_dir)
                
                # Format t√™n file: Timestamp.jpg
                filename = f"{int(time.time())}.jpg"
                save_path = os.path.join(student_collect_dir, filename)
                
                # L∆∞u ·∫£nh x√°c th·ª±c (S·ª≠ d·ª•ng b·∫£n clean_snapshot s·∫°ch)
                with state.lock:
                    target_image = state.clean_snapshot if state.clean_snapshot is not None else state.frame
                    if target_image is not None:
                        cv2.imwrite(save_path, target_image)
                        print(f"üì∏ ƒê√£ l∆∞u ·∫£nh S·∫†CH v√†o folder t·ª± h·ªçc: {save_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói l∆∞u ·∫£nh t·ª± h·ªçc: {e}")
            # ----------------------------------------
            
            print(f"CONFIRMED: {sid}")
    
    # Reset state ngay l·∫≠p t·ª©c
    with state.lock:
        state.status = "SCANNING"
        state.student_data = None
        state.last_scan_time = time.time() + 0.5 # Delay 0.5s tr∆∞·ªõc khi qu√©t l·∫°i (M∆∞·ª£t h∆°n)
        
    return jsonify({"success": True})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
