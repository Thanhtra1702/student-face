from flask import Flask, render_template, Response, jsonify, request
import cv2
import threading
import time
import datetime
import os
import numpy as np
from deepface import DeepFace
from qdrant_client import QdrantClient

# Import DB handler t·ª´ module c≈©
from kiosk_db import DatabaseHandler

import mediapipe as mp

import signal
import sys

app = Flask(__name__)

# --- PRELOAD MODEL (ƒê·ªÉ kh·ªüi ƒë·ªông nhanh h∆°n) ---
print("üöÄ ƒêang t·∫£i model AI...")
try:
    # Preload ArcFace model b·∫±ng c√°ch t·∫°o embedding gi·∫£
    import numpy as np
    dummy_img = np.zeros((112, 112, 3), dtype=np.uint8)
    DeepFace.represent(img_path=dummy_img, model_name="ArcFace", detector_backend="skip", enforce_detection=False)
    print("‚úÖ Model AI ƒë√£ s·∫µn s√†ng!")
except:
    pass

# --- QDRANT CLIENT ---
QDRANT_PATH = "./qdrant_db"
COLLECTION_NAME = "student_faces"
# qdrant_client = QdrantClient(path=QDRANT_PATH) # REMOVED to avoid double locking

# --- GLOBAL STATE ---
class KioskState:
    def __init__(self):
        self.frame = None
        self.lock = threading.Lock()
        self.status = "SCANNING" # SCANNING, LIVENESS, PROCESSING, CONFIRM, SPOOF
        self.progress = 0
        self.student_data = None 
        self.last_scan_time = 0
        self.process_start_time = 0
        self.db = DatabaseHandler()
        self.running = True # C·ªù ki·ªÉm so√°t v√≤ng l·∫∑p
        # Liveness Blink State
        self.blink_counter = 0
        self.is_blinking = False
        self.last_blink_time = 0  # Th·ªùi gian l·∫ßn ch·ªõp tr∆∞·ªõc (ƒë·ªÉ ch·ªëng video replay)
        # Verification State
        self.consecutive_match_count = 0
        self.last_recognized_sid = None
        
state = KioskState()

# Handle Ctrl+C
def signal_handler(sig, frame):
    print('üëã ƒêang t·∫Øt h·ªá th·ªëng NGAY L·∫¨P T·ª®C...')
    state.running = False
    # time.sleep(0.5)  <-- X√≥a d√≤ng n√†y
    os._exit(0)  # Force exit ngay l·∫≠p t·ª©c

signal.signal(signal.SIGINT, signal_handler)

# --- BLINK DETECTION HELPERS ---
mp_face_mesh = mp.solutions.face_mesh
# Index m·∫Øt tr√°i/ph·∫£i trong FaceMesh (Chu·∫©n Mediapipe)
LEFT_EYE = [362, 385, 387, 263, 373, 380]
RIGHT_EYE = [33, 160, 158, 133, 153, 144]

def calculate_ear(landmarks, eye_indices, w, h):
    """T√≠nh Eye Aspect Ratio (T·ª∑ l·ªá m·ªü m·∫Øt)"""
    # L·∫•y t·ªça ƒë·ªô
    coords = []
    for idx in eye_indices:
        lm = landmarks[idx]
        coords.append((lm.x * w, lm.y * h))
    
    # T√≠nh kho·∫£ng c√°ch d·ªçc (Vertical)
    v1 = np.linalg.norm(np.array(coords[1]) - np.array(coords[5]))
    v2 = np.linalg.norm(np.array(coords[2]) - np.array(coords[4]))
    
    # T√≠nh kho·∫£ng c√°ch ngang (Horizontal)
    h_dist = np.linalg.norm(np.array(coords[0]) - np.array(coords[3]))
    
    ear = (v1 + v2) / (2.0 * h_dist)
    return ear

# --- AI ENHANCEMENT HELPERS ---
def preprocess_frame(frame):
    """
    Gi·ªØ nguy√™n frame g·ªëc cho AI x·ª≠ l√Ω.
    C√°c model hi·ªán ƒë·∫°i (ArcFace) ho·∫°t ƒë·ªông t·ªët nh·∫•t v·ªõi d·ªØ li·ªáu g·ªëc thay v√¨ filter th·ªß c√¥ng.
    """
    return frame

def check_img_quality(frame):
    # T·∫Øt check ch·∫•t l∆∞·ª£ng qu√° g·∫Øt ƒë·ªÉ tr√°nh ch·∫∑n nh·∫ßm trong m√¥i tr∆∞·ªùng t·ªëi
    return True, "OK"

def check_spoofing_opencv(frame, face_area=None):
    return False, "Real"

def run_recognition_async(frame, state):
    """Ch·∫°y AI Nh·∫≠n di·ªán - Debug Mode (RGB + Low Threshold)"""
    print(f"üöÄ AI Start: K√≠ch th∆∞·ªõc ·∫£nh {frame.shape}")
    try:
        # DeepFace l√†m vi·ªác t·ªët chu·∫©n v·ªõi RGB
        input_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # --- 1. DETECT & EXTRACT FACE ---
        print("üîç ƒêang Detect Face (Mediapipe)...")
        face_objs = DeepFace.extract_faces(
            img_path=input_frame,
            detector_backend="mediapipe",
            enforce_detection=True,
            align=True,
            grayscale=False
        )
        
        if not face_objs:
            print("‚ö†Ô∏è DeepFace kh√¥ng t√¨m th·∫•y m·∫∑t")
            with state.lock:
                state.status = "SCANNING"
                state.progress = 0
            return

        current_face = face_objs[0]["face"]
        print(f"‚úÖ Face Detected. Shape: {current_face.shape}")

        if current_face.max() <= 1.0:
            current_face = (current_face * 255).astype(np.uint8)

        # --- 2. GET EMBEDDING ---
        print("üß¨ ƒêang t·∫°o Embedding (ArcFace)...")
        results = DeepFace.represent(
            img_path=current_face,
            model_name="ArcFace",
            detector_backend="skip",
            enforce_detection=False,
            align=True
        )
        
        if not results: 
            print("‚ùå L·ªói t·∫°o Embedding")
            with state.lock:
                state.status = "SCANNING"
                state.progress = 0
            return
            
        embedding = results[0]["embedding"]
        
        # --- 3. SEARCH DATABASE ---
        print("üîé ƒêang Query Qdrant...")
        search_res = state.db.client.query_points(
            collection_name=COLLECTION_NAME,
            query=embedding,
            limit=3
        ).points
        
        found = False
        
        if search_res:
            best_match = search_res[0]
            score = best_match.score
            print(f"üéØ Top 1: {best_match.payload['student_id']} - Score: {score:.4f}")
            
            accepted_sid = None
            
            # --- 4. MATCHING LOGIC (SMART GAP CHECK) ---
            # H·∫° xu·ªëng 0.40
            if score > 0.40:
                # T√¨m ƒë·ªëi th·ªß th·ª±c s·ª± (ng∆∞·ªùi ƒë·∫ßu ti√™n c√≥ ID kh√°c)
                competitor = None
                for res in search_res[1:]:
                    if res.payload['student_id'] != best_match.payload['student_id']:
                        competitor = res
                        break
                
                if competitor:
                    gap = score - competitor.score
                    print(f"   Gap vs Different Person ({competitor.payload['student_id']}: {competitor.score:.4f}): {gap:.4f}")
                    
                    # N·∫øu ph√¢n v√¢n gi·ªØa 2 ng∆∞·ªùi kh√°c nhau m√† kho·∫£ng c√°ch qu√° h·∫πp (< 0.02)
                    if gap < 0.02 and score < 0.65:
                         print(f"‚ö†Ô∏è T·ª´ ch·ªëi: Nh·∫≠p nh·∫±ng gi·ªØa {best_match.payload['student_id']} v√† {competitor.payload['student_id']}")
                    else:
                        accepted_sid = best_match.payload['student_id']
                else:
                    # Kh√¥ng c√≥ ƒë·ªëi th·ªß kh√°c ID n√†o trong top -> Qu√° an to√†n
                    accepted_sid = best_match.payload['student_id']
            
            if accepted_sid:
                print(f"‚úÖ CH·∫§P NH·∫¨N MATCH: {accepted_sid}")
                
                # B·ªè qua Consecutive check ƒë·ªÉ test ƒë·ªô nh·∫°y -> Confirm L·∫≠p T·ª©c
                name, sch, room = state.db.get_student_info(accepted_sid)
                with state.lock:
                    state.student_data = {
                        "name": name,
                        "student_id": accepted_sid,
                        "schedule": sch, # Map data
                        "room": room,
                        "checkin_time": datetime.datetime.now().strftime("%H:%M %d/%m")
                    }
                    state.status = "CONFIRM"
                    state.progress = 100
                    state.consecutive_match_count = 0 
                found = True
            else:
                print(f"‚ùå Low Score (< 0.40) ho·∫∑c Ambiguous")
        else:
            print("‚ùå DB Empty")
        
        if not found:
            with state.lock:
                state.status = "SCANNING"
                state.progress = 0

    except Exception as e:
        print(f"üî• AI Exception: {e}")
        import traceback
        traceback.print_exc()
        with state.lock:
            state.status = "SCANNING"
            state.progress = 0
        return

        # QUAN TR·ªåNG: N·∫øu ch∆∞a Confirm v√† ch∆∞a v·ªÅ Scanning -> Reset timer ƒë·ªÉ Camera Worker g·ªçi ti·∫øp
        if state.status == "PROCESSING":
            with state.lock:
                # ƒê·∫∑t l·∫°i th·ªùi gian ƒë·ªÉ camera worker ti·∫øp t·ª•c ƒë·∫øm process
                # Tr·ª´ ƒëi 0.3s ƒë·ªÉ l·∫ßn sau ch·∫°y nhanh h∆°n (ch·ªâ ƒë·ª£i 0.2s)
                state.process_start_time = time.time() - 0.3

    except Exception as e:
        print(f"AI Error: {e}")
        with state.lock:
            state.status = "SCANNING"
            state.progress = 0

# --- CAMERA THREAD ---
def camera_worker():
    cap = cv2.VideoCapture(0)
    # Gi·∫£m ƒë·ªô ph√¢n gi·∫£i xu·ªëng 640x480 ƒë·ªÉ m∆∞·ª£t h∆°n
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    # Init Face Mesh (V·∫´n d√πng ƒë·ªÉ detect khu√¥n m·∫∑t nhanh)
    face_mesh = mp_face_mesh.FaceMesh(
        max_num_faces=1,
        refine_landmarks=True,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )

    while state.running:
        ret, frame = cap.read()
        if not ret: break
        frame = cv2.flip(frame, 1)
        
        # --- LOCK AI WHEN CONFIRMING (NEW) ---
        # N·∫øu ƒëang ƒë·ª£i ng∆∞·ªùi d√πng b·∫•m n√∫t, kh√¥ng l√†m g√¨ c·∫£ ƒë·ªÉ ti·∫øt ki·ªám CPU v√† tr√°nh nh·∫£y log
        if state.status == "CONFIRM":
            with state.lock:
                state.frame = frame.copy()
            time.sleep(0.1)
            continue
            
        # --- STATE MACHINE ---
        current_time = time.time()
        
        # --- MAIN FACE SELECTION (Anti-Crowd) ---
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb_frame)
        
        if results.multi_face_landmarks:
            h, w, _ = frame.shape
            screen_center_x, screen_center_y = w // 2, h // 2
            
            best_face_data = None
            max_focus_score = -1

            # Duy·ªát qua t·∫•t c·∫£ m·∫∑t ƒë·ªÉ t√¨m "Ng∆∞·ªùi ch·ªß tr√¨"
            for face_landmarks in results.multi_face_landmarks:
                # T√≠nh bounding box
                x_min, y_min = w, h
                x_max, y_max = 0, 0
                for lm in face_landmarks.landmark:
                    cx, cy = int(lm.x * w), int(lm.y * h)
                    x_min, y_min = min(x_min, cx), min(y_min, cy)
                    x_max, y_max = max(x_max, cx), max(y_max, cy)
                
                # T√≠nh ƒëi·ªÉm ∆∞u ti√™n (Di·ªán t√≠ch * ƒê·ªô trung t√¢m)
                area = (x_max - x_min) * (y_max - y_min)
                face_center_x = (x_min + x_max) / 2
                face_center_y = (y_min + y_max) / 2
                dist_to_center = ((face_center_x - screen_center_x)**2 + (face_center_y - screen_center_y)**2)**0.5
                
                # Heuristic: ∆Øu ti√™n m·∫∑t TO v√† G·∫¶N T√ÇM (Tr·ªçng s·ªë di·ªán t√≠ch cao h∆°n)
                focus_score = area / (dist_to_center + 1) 
                
                if focus_score > max_focus_score:
                    max_focus_score = focus_score
                    best_face_data = (x_min, y_min, x_max, y_max)

            # Ch·ªâ v·∫Ω v√† x·ª≠ l√Ω khu√¥n m·∫∑t T·ªêT NH·∫§T
            if best_face_data:
                x_min, y_min, x_max, y_max = best_face_data
                
                # V·∫Ω box (M√†u Cam n·∫øu ƒëang Scan/Process, M√†u Xanh n·∫øu ƒë√£ Confirm)
                color = (33, 111, 242) # FPT Orange
                if state.status == "CONFIRM": color = (73, 132, 30) # Green
                
                # V·∫Ω g√≥c Corner
                t, l = 2, 30
                cv2.line(frame, (x_min, y_min), (x_min + l, y_min), color, t + 2)
                cv2.line(frame, (x_min, y_min), (x_min, y_min + l), color, t + 2)
                cv2.line(frame, (x_max, y_min), (x_max - l, y_min), color, t + 2)
                cv2.line(frame, (x_max, y_min), (x_max, y_min + l), color, t + 2)
                cv2.line(frame, (x_min, y_max), (x_min + l, y_max), color, t + 2)
                cv2.line(frame, (x_min, y_max), (x_min, y_max - l), color, t + 2)
                cv2.line(frame, (x_max, y_max), (x_max - l, y_max), color, t + 2)
                cv2.line(frame, (x_max, y_max), (x_max, y_max - l), color, t + 2)
                
                # Trigger Processing
                if state.status == "SCANNING" and (current_time - state.last_scan_time > 1.0):
                    with state.lock:
                        state.status = "PROCESSING"
                        state.process_start_time = current_time
                        state.progress = 0

        # 2. PROCESSING logic (S·ª≠ d·ª•ng frame ƒë√£ v·∫Ω box l√†m preview)
        if state.status == "PROCESSING":
            elapsed = current_time - state.process_start_time
            if elapsed < 0:
                with state.lock: state.progress = 90 + int((current_time * 10) % 9)
            else:
                prog = int((elapsed / 0.5) * 90)
                with state.lock: state.progress = min(90, max(0, prog))
                if elapsed > 0.3:
                    # Ch·ª•p frame g·ªëc ƒë·ªÉ x·ª≠ l√Ω AI
                    threading.Thread(target=run_recognition_async, args=(frame.copy(), state), daemon=True).start()
                    with state.lock: state.process_start_time = current_time + 1000 

        with state.lock:
            state.frame = frame.copy()
        
        time.sleep(0.01)

# Start Thread
t = threading.Thread(target=camera_worker, daemon=True)
t.start()

# --- WEB ROUTES ---
@app.route('/')
def index():
    return render_template('index.html')

def generate_frames():
    while True:
        with state.lock:
            if state.frame is None: continue
            
            # encode frame
            _, buffer = cv2.imencode('.jpg', state.frame)
            frame_bytes = buffer.tobytes()
            
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')

@app.route('/video_feed')
def video_feed():
    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/api/status')
def get_status():
    return jsonify({
        "status": state.status,
        "progress": state.progress,
        "data": state.student_data
    })

@app.route('/api/action', methods=['POST'])
def handle_action():
    req = request.json
    action = req.get('action') # 'confirm' or 'reject'
    
    if action == 'confirm':
        # Logic L∆∞u ƒëi·ªÉm danh (ƒê√£ c√≥)
        if state.student_data:
            sid = state.student_data['student_id']
            
            # --- T√çNH NƒÇNG T·ª∞ H·ªåC (SELF-LEARNING) ---
            # L∆∞u ·∫£nh x√°c th·ª±c v√†o collected_faces/MSSV/ ƒë·ªÉ ƒë·ªãnh k·ª≥ training l·∫°i
            try:
                # T·∫°o folder ri√™ng cho t·ª´ng sinh vi√™n
                student_collect_dir = os.path.join("collected_faces", sid)
                if not os.path.exists(student_collect_dir):
                    os.makedirs(student_collect_dir)
                
                # Format t√™n file: Timestamp.jpg
                filename = f"{int(time.time())}.jpg"
                save_path = os.path.join(student_collect_dir, filename)
                
                # L∆∞u frame t·∫°i th·ªùi ƒëi·ªÉm x√°c nh·∫≠n
                with state.lock:
                    if state.frame is not None:
                        cv2.imwrite(save_path, state.frame)
                        print(f"üì∏ ƒê√£ l∆∞u ·∫£nh t·ª± h·ªçc v√†o folder: {save_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói l∆∞u ·∫£nh t·ª± h·ªçc: {e}")
            # ----------------------------------------
            
            print(f"CONFIRMED: {sid}")
    
    # Reset state ngay l·∫≠p t·ª©c
    with state.lock:
        state.status = "SCANNING"
        state.student_data = None
        state.last_scan_time = time.time() + 0.5 # Delay 0.5s tr∆∞·ªõc khi qu√©t l·∫°i (M∆∞·ª£t h∆°n)
        
    return jsonify({"success": True})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
